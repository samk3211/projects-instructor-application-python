{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tests\n",
    "\n",
    "The next step for creating a DataCamp project in Python is creating a few tests using the `nose` testing framework, which is how DataCamp instructors deliver feedback on the code students write in a project.\n",
    "\n",
    "After installing the necessary libraries (described below), please create tests for the project tasks below, which were taken from real live DataCamp projects!\n",
    "\n",
    "When complete, please email the link to your forked repo to projects@datacamp.com with the email subject line _DataCamp project tests_. If you have any questions, please reach out to projects@datacamp.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to run tests locally in the notebook, install the following:\n",
    "# pip install nose\n",
    "# pip install git+https://github.com/datacamp/ipython_nose\n",
    "\n",
    "# Then load in the ipython_nose extension like this:\n",
    "%load_ext ipython_nose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of a `nose` test\n",
    "\n",
    "Instructions to the student in the project:\n",
    "- Create a list of six to ten strings named `words` that contain words related to selling discount furniture online.\n",
    "\n",
    "A potential **incorrect** submission is as follows. Please process the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['buy', 'price', 'discount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the `nose` test below that tests the length of the list, then run the test locally. Processing the cell above followed by the cell below will run the test locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": "{\"success\": false, \"summary\": {\"tests\": 1, \"failures\": 1, \"errors\": 0}, \"tests\": [{\"name\": \"__main__.test_task_1\", \"success\": false, \"message\": \"AssertionError: There should be six to ten brief-related words in the list `words`.\\n\"}]}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0/1 tests passed; 1 failed\n",
       "========\n",
       "__main__.test_task_1\n",
       "========\n",
       "Traceback (most recent call last):\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\unittest\\case.py\", line 59, in testPartExecutor\n",
       "    yield\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\unittest\\case.py\", line 615, in run\n",
       "    testMethod()\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nose\\case.py\", line 197, in runTest\n",
       "    self.test(*self.arg)\n",
       "  File \"<string>\", line 3, in test_task_1\n",
       "AssertionError: There should be six to ten brief-related words in the list `words`.\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%nose # needed at the start of every tests cell\n",
    "def test_task_1():\n",
    "    assert 6 <= len(set(words)) <= 10, \\\n",
    "    \"There should be six to ten brief-related words in the list `words`.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A potential **correct** submission is as follows. Please process the cell below to overwrite the correct `words` solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['buy', 'price', 'discount', 'promotion', 'promo', 'shop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": "{\"success\": true, \"summary\": {\"tests\": 1, \"failures\": 0, \"errors\": 0}, \"tests\": [{\"name\": \"__main__.test_task_1\", \"success\": true, \"message\": \"\"}]}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1/1 tests passed\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%nose # needed at the start of every tests cell\n",
    "def test_task_1():\n",
    "    assert 6 <= len(set(words)) <= 10, \\\n",
    "    \"There should be six to ten brief-related words in the list `words`.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your first test\n",
    "Instructions to the student in the project:\n",
    "- Import the class `Image` from the library `PIL`.\n",
    "\n",
    "A potential **incorrect** submission is as follows. Please process the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-47ac575cf81a>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-47ac575cf81a>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    import Image from PIL\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# import the Image class\n",
    "import Image from PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the `nose` test template to test if `Image` exists in `globals()` and include a helpful feedback message for failing submissions. The test should fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": "{\"success\": false, \"summary\": {\"tests\": 1, \"failures\": 1, \"errors\": 0}, \"tests\": [{\"name\": \"__main__.test_Image_loaded\", \"success\": false, \"message\": \"AssertionError: The Image class should be imported from the PIL library. Define 'from' before 'import'\\n\"}]}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0/1 tests passed; 1 failed\n",
       "========\n",
       "__main__.test_Image_loaded\n",
       "========\n",
       "Traceback (most recent call last):\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\unittest\\case.py\", line 59, in testPartExecutor\n",
       "    yield\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\unittest\\case.py\", line 615, in run\n",
       "    testMethod()\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nose\\case.py\", line 197, in runTest\n",
       "    self.test(*self.arg)\n",
       "  File \"<string>\", line 3, in test_Image_loaded\n",
       "AssertionError: The Image class should be imported from the PIL library. Define 'from' before 'import'\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%nose # needed at the start of every tests cell\n",
    "def test_Image_loaded():\n",
    "    assert \"Image\" in globals(), \\\n",
    "    \"The Image class should be imported from the PIL library. Define 'from' before 'import'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A potential **correct** solution is as follows. Please process the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Image class\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please copy and paste the test you just wrote into the cell below and process it. The test should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": "{\"success\": true, \"summary\": {\"tests\": 1, \"failures\": 0, \"errors\": 0}, \"tests\": [{\"name\": \"__main__.test_Image_loaded\", \"success\": true, \"message\": \"\"}]}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1/1 tests passed\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%nose # needed at the start of every tests cell\n",
    "def test_Image_loaded():\n",
    "    assert \"Image\" in globals(), \\\n",
    "    \"The Image class should be imported from PIL. Define from before the import\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your second test\n",
    "\n",
    "Instructions to the student in the project:\n",
    "- Import pandas aliased as `pd`.\n",
    "- Load `datasets/nobel.csv` into a DataFrame and assign it to the variable `nobel`.\n",
    "\n",
    "A potential **incorrect** submission is as follows. Please process the cell below. *Note: `nobel.csv` exists in a directory named `datasets` in the same directory as this `create_tests.ipynb` notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "import pandas\n",
    "\n",
    "# read in the dataset\n",
    "nobel = pandas.read_csv('datasets/nobel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the two `nose` test templates below to test if `pd` exists in `globals()` and if `nobel` is correctly loaded (using the [`equals()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.equals.html) method of a pandas DataFrame). Include a helpful feedback message for failing submissions. Both tests should fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": "{\"success\": false, \"summary\": {\"tests\": 2, \"failures\": 1, \"errors\": 1}, \"tests\": [{\"name\": \"__main__.test_pandas_loaded\", \"success\": false, \"message\": \"AssertionError: Import pandas aliased as pd\\n\"}, {\"name\": \"__main__.test_nobel_loaded\", \"success\": false, \"message\": \"NameError: name 'pd' is not defined\\n\"}]}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0/2 tests passed; 2 failed\n",
       "========\n",
       "__main__.test_pandas_loaded\n",
       "========\n",
       "Traceback (most recent call last):\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\unittest\\case.py\", line 59, in testPartExecutor\n",
       "    yield\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\unittest\\case.py\", line 615, in run\n",
       "    testMethod()\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nose\\case.py\", line 197, in runTest\n",
       "    self.test(*self.arg)\n",
       "  File \"<string>\", line 4, in test_pandas_loaded\n",
       "AssertionError: Import pandas aliased as pd\n",
       "\n",
       "========\n",
       "__main__.test_nobel_loaded\n",
       "========\n",
       "Traceback (most recent call last):\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\unittest\\case.py\", line 59, in testPartExecutor\n",
       "    yield\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\unittest\\case.py\", line 615, in run\n",
       "    testMethod()\n",
       "  File \"C:\\Users\\sakhal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nose\\case.py\", line 197, in runTest\n",
       "    self.test(*self.arg)\n",
       "  File \"<string>\", line 7, in test_nobel_loaded\n",
       "NameError: name 'pd' is not defined\n",
       "\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%nose # needed at the start of every tests cell\n",
    "\n",
    "def test_pandas_loaded():\n",
    "    assert \"pd\" in globals(), \\\n",
    "    \"Import pandas aliased as pd\"\n",
    "\n",
    "    import pandas\n",
    "\n",
    "def test_nobel_loaded():\n",
    "    correct_nobel = pd.read_csv(\"datasets/nobel.csv\")\n",
    "    try:\n",
    "        pd.testing.assert_frame_equal(nobel, correct_nobel)\n",
    "    except AssertionError:\n",
    "        assert False, \"The variable nobel should contain the data in datasets/nobel.csv. Load it using pandas.\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A potential **correct** solution is as follows. Please process the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "import pandas as pd\n",
    "\n",
    "# read in the dataset\n",
    "nobel = pd.read_csv('datasets/nobel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please copy and paste the tests you just wrote into the cell below and process the cell. Both tests should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nose # needed at the start of every tests cell\n",
    "\n",
    "def test_pandas_loaded():\n",
    "    assert \"pd\" in globals(), \\\n",
    "    \"Import pandas aliased as pd\"\n",
    "\n",
    "def test_nobel_loaded():\n",
    "    correct_nobel = pd.read_csv(\"datasets/nobel.csv\")\n",
    "    try:\n",
    "        pd.testing.assert_frame_equal(nobel, correct_nobel)\n",
    "    except AssertionError:\n",
    "        assert False, \"The variable nobel should contain the data in datasets/nobel.csv. Load it using pandas.\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
